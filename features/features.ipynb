{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"features.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"HR9MnBNA_rCA","colab_type":"code","colab":{}},"source":["# In this notebook we create the features that will be used in the modeling phase\n","# So, we first read the files that we created in the preprocessing phase\n","# Finally we save 3 pickle files with features:\n","# 1) df_train_ptstkwat.pkl - it has several numeric features\n","# 2) df_similarities.pkl - it has features based on similarities (Levenshtein, Cosine, Jaccard, Jaro)\n","# 3) df_fuzzy.pkl - it has features based on this --> https://towardsdatascience.com/natural-language-processing-for-fuzzy-string-matching-with-python-6632b7824c49"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sG3puYqr_rCD","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","# pip stringdist\n","\n","# import stringdist\n","\n","from collections import Counter\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# print non truncated column info in pandas dataframe\n","pd.set_option('display.max_colwidth', -1)\n","pd.set_option('display.max_columns', 500)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8iI61fEB_rCH","colab_type":"code","colab":{}},"source":["# read the preprocessed trainset\n","df_train = pd.read_pickle(r'C:\\Users\\Dimos\\Desktop\\MSc\\Semester 4\\NLP\\Coursework\\data\\preprocessed\\df_train_prep.pkl')\n","# read the file with the preprocessed descriptions and their keywords\n","df_descs = pd.read_pickle(r'C:\\Users\\Dimos\\Desktop\\MSc\\Semester 4\\NLP\\Coursework\\data\\preprocessed\\df_descr_kwd.pkl')\n","# read the file with the preprocessed attributes\n","df_attrs = pd.read_pickle(r'C:\\Users\\Dimos\\Desktop\\MSc\\Semester 4\\NLP\\Coursework\\data\\preprocessed\\df_attrs_prep.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bqpO5WP_rCL","colab_type":"code","colab":{}},"source":["# merge the above dataframes\n","df_train = df_train.merge(df_descs, left_on='product_uid', right_on='product_uid', how='left')\n","df_train = df_train.merge(df_attrs, left_on='product_uid', right_on='product_uid', how='left')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6ZV6FnA_rCP","colab_type":"text"},"source":["## - Features from TRAINSET + Descriptions + Attributes"]},{"cell_type":"code","metadata":{"id":"xyGLYEVe_rCQ","colab_type":"code","colab":{}},"source":["df_train.head(2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xy2TS-DI_rCT","colab_type":"text"},"source":["### -- General counts about numerics and non numerics in Product title and Search term"]},{"cell_type":"code","metadata":{"id":"JCUp7XTZ_rCU","colab_type":"code","colab":{}},"source":["# number of numeric terms in product_title\n","df_train['N_numerics_PT'] = df_train['PT_numerics'].apply(lambda x: len(x))\n","# number of numeric terms in search_term\n","df_train['N_numerics_ST'] = df_train['ST_numerics'].apply(lambda x: len(x))\n","\n","# number of non numeric terms in product_title\n","df_train['N_non_numerics_PT'] = df_train['PT_Non_numerics'].apply(lambda x: len(x))\n","# number of non numeric terms in search_term\n","df_train['N_non_numerics_ST'] = df_train['ST_Non_numerics'].apply(lambda x: len(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"slIw73XA_rCX","colab_type":"code","colab":{}},"source":["# DONT INCLUDE IT\n","\n","# number of common words between 'product_title' & 'search_term'\n","# df_train['N_common_words'] = df_train.apply(lambda x: \n","#                             len(list(set(x['product_title_tokens']).intersection(x['search_term_tokens']))), axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JL-BeDZl_rCb","colab_type":"text"},"source":["### -- Common terms between Search term & Product title"]},{"cell_type":"code","metadata":{"id":"OSRIh7xf_rCd","colab_type":"code","colab":{}},"source":["%%time\n","\n","def common_words_leven(tokens_1, tokens_2):\n","    #N = 0\n","    common_terms = []\n","    tokens_1 = list(set(tokens_1))\n","    tokens_2 = list(set(tokens_2))\n","    \n","    for token1 in tokens_1:\n","        for token2 in tokens_2:\n","            if 1 - stringdist.levenshtein_norm(token1, token2)>0.85:\n","                #N += 1\n","                common_terms.append(token2)\n","    try:\n","        return common_terms\n","    except:\n","        return common_terms\n","\n","# number of common nonnumeric terms between 'PT_Non_numerics' & 'ST_Non_numerics' with levensthein distance\n","df_train['Common_words_leven'] = df_train.apply(lambda x: common_words_leven(x['PT_Non_numerics'], x['ST_Non_numerics']),\n","                                                                              axis=1)\n","\n","df_train['N_common_words_leven'] = df_train['Common_words_leven'].apply(lambda x: len(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNmnfz9L_rCi","colab_type":"code","colab":{}},"source":["# Jaccard similarity based on the above common words with levensthein\n","def get_jaccard_sim(words1, words2, common): \n","    \"\"\"Returns jaccard similarity between 2 list of words\"\"\"\n","    a = set(words1)\n","    b = set(words2)\n","    c = set(common)\n","    return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","df_train['JC_sim'] = df_train.apply(lambda x: get_jaccard_sim(x['PT_Non_numerics'], x['ST_Non_numerics'], x['Common_words_leven']),\n","                                         axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_z_IOvnm_rCk","colab_type":"code","colab":{}},"source":["# DONT INCLUDE IT\n","\n","# number of common stemmed words between 'product_title_stem' & 'search_term_stem'\n","# df_train['N_common_words_stem'] = df_train.apply(lambda x: \n","#                             len(list(set(x['product_title_stem']).intersection(x['search_term_stem']))), axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kfw-7iCd_rCp","colab_type":"text"},"source":["### -- Non numeric terms of Search term that are substrings of Product title | Descrtiption | Attributes "]},{"cell_type":"code","metadata":{"id":"BWnXBlGd_rCq","colab_type":"code","colab":{}},"source":["# number of non numeric terms of the search_term that appears in the Product title | Descrtiption | Attributes\n","\n","def n_substrings(tokens, text):\n","    #N = 0\n","    substrings = []\n","    for token in tokens:\n","        try:\n","            if token in text:\n","                #N += 1\n","                substrings.append(token)\n","        except:\n","            # in case the text is float\n","            pass\n","    return substrings\n","\n","##### PRODUCT TITLE \n","# list of terms of search_term_tokens that are substrings of PT_lower\n","df_train['Substrs_PT_x'] = df_train.apply(lambda x: n_substrings(x['ST_Non_numerics'], x['PT_lower']), axis=1)\n","\n","# Number of terms of search_term_tokens that are substrings of product_title_lower\n","df_train['N_substrs_PT_x'] = df_train['Substrs_PT_x'].apply(lambda x: len(x))\n","\n","\n","#### PRODUCT DESCRIPTION\n","# list of terms of search_term_tokens that are substrings of PD_lower\n","df_train['Substrs_PD_x'] = df_train.apply(lambda x: n_substrings(x['ST_Non_numerics'], x['PD_lower']), axis=1)\n","\n","# Number of terms of search_term_tokens that are substrings of product_title_lower\n","df_train['N_substrs_PD_x'] = df_train['Substrs_PD_x'].apply(lambda x: len(x))\n","\n","#### PRODUCT ATTRIBUTES\n","# list of terms of search_term_tokens that are substrings of PD_lower\n","df_train['Substrs_Atr_x'] = df_train.apply(lambda x: n_substrings(x['ST_Non_numerics'], x['Atrr_text']), axis=1)\n","\n","# Number of terms of search_term_tokens that are substrings of product_title_lower\n","df_train['N_substrs_Atr_x'] = df_train['Substrs_Atr_x'].apply(lambda x: len(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xGb2F62_rCs","colab_type":"code","colab":{}},"source":["def perc_xxx(tokens1, tokens2, tokens3, tokens4):\n","    try:\n","        tokens123 = list(set(tokens1 + tokens2 + tokens3))\n","        return len(tokens123)/len(tokens4)\n","    except:\n","        return 0\n","\n","# percentage of terms of search_term_tokens that are substrings of PT_lower or PD_lower or Atrr_text\n","df_train['Perc_substrs_x'] = df_train.apply(lambda x: perc_xxx(x['Substrs_PT_x'],x['Substrs_PD_x'],x['Substrs_Atr_x'],x['ST_Non_numerics']),\n","                                          axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6eFK6EC_rCx","colab_type":"text"},"source":["### -- Numeric terms of Search term that are substrings of Product title | Descrtiption | Attributes "]},{"cell_type":"code","metadata":{"id":"eAZybCVM_rCy","colab_type":"code","colab":{}},"source":["# number of numeric terms of the search_term that appears in the Product title | Descrtiption | Attributes\n","def n_substrings(tokens, text):\n","    #N = 0\n","    substrings = []\n","    for token in tokens:\n","        try:\n","            if token in text:\n","                #N += 1\n","                substrings.append(token)\n","        except:\n","            # in case the text is float\n","            pass\n","    return substrings\n","\n","##### PRODUCT TITLE \n","# list of terms of search_term_tokens that are substrings of PT_lower\n","df_train['Substrs_PT_y'] = df_train.apply(lambda x: n_substrings(x['ST_numerics'], x['PT_lower']), axis=1)\n","\n","# Number of terms of search_term_tokens that are substrings of product_title_lower\n","df_train['N_substrs_PT_y'] = df_train['Substrs_PT_y'].apply(lambda x: len(x))\n","\n","\n","#### PRODUCT DESCRIPTION\n","# list of terms of search_term_tokens that are substrings of PD_lower\n","df_train['Substrs_PD_y'] = df_train.apply(lambda x: n_substrings(x['ST_numerics'], x['PD_lower']), axis=1)\n","\n","# Number of terms of search_term_tokens that are substrings of product_title_lower\n","df_train['N_substrs_PD_y'] = df_train['Substrs_PD_y'].apply(lambda x: len(x))\n","\n","#### PRODUCT ATTRIBUTES\n","# list of terms of search_term_tokens that are substrings of PD_lower\n","df_train['Substrs_Atr_y'] = df_train.apply(lambda x: n_substrings(x['ST_numerics'], x['Atrr_text']), axis=1)\n","\n","# Number of terms of search_term_tokens that are substrings of product_title_lower\n","df_train['N_substrs_Atr_y'] = df_train['Substrs_Atr_y'].apply(lambda x: len(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6o7hfw3_rC0","colab_type":"code","colab":{}},"source":["def perc_xxx(tokens1, tokens2, tokens3, tokens4):\n","    try:\n","        tokens123 = list(set(tokens1 + tokens2 + tokens3))\n","        return len(tokens123)/len(tokens4)\n","    except:\n","        return 0\n","\n","# percentage of terms of search_term_tokens that are substrings of PT_lower or PD_lower or Atrr_text\n","df_train['Perc_substrs_y'] = df_train.apply(lambda x: perc_xxx(x['Substrs_PT_y'],x['Substrs_PD_y'],x['Substrs_Atr_y'],x['ST_numerics']),\n","                                          axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TinqxXJq_rC3","colab_type":"text"},"source":["### -- Levensthein distance between Search terms & Product title"]},{"cell_type":"code","metadata":{"id":"LSrGAZye_rC4","colab_type":"code","colab":{}},"source":["# Levensthein distance between 'product_title_text' & 'search_term_text'\n","df_train['Leven_sim_ST_PT'] = df_train.apply(lambda x: 1 - stringdist.levenshtein_norm(x['PT_text'], x['ST_text']),\n","                                         axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PcVDrkmO_rC6","colab_type":"text"},"source":["### -- Keywords of Description that appear in the Search term (with levensthein distance)"]},{"cell_type":"code","metadata":{"id":"OC91zq6__rC9","colab_type":"code","colab":{}},"source":["%%time\n","\n","def common_words_leven(tokens_1, tokens_2):\n","    #N = 0\n","    common_terms = []\n","    tokens_1 = list(set(tokens_1))\n","    tokens_2 = list(set(tokens_2))\n","    \n","    for token1 in tokens_1:\n","        for token2 in tokens_2:\n","            if 1 - stringdist.levenshtein_norm(token1, token2)>0.85:\n","                #N += 1\n","                common_terms.append(token1)\n","    try:\n","        return common_terms\n","    except:\n","        return common_terms\n","\n","# list of Descripton Keywords that appear in the 'ST_Non_numerics' with levensthein distance\n","df_train['Keywords_leven'] = df_train.apply(lambda x: common_words_leven(x['Keywords_Descr'], x['ST_Non_numerics']),\n","                                                                              axis=1)\n","# number of Descripton Keywords that appear in the 'ST_Non_numerics' with levensthein distance\n","df_train['N_keywords_leven'] = df_train['Keywords_leven'].apply(lambda x: len(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqSDq5j3_rDI","colab_type":"code","colab":{}},"source":["df_train.columns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJhAWhLm_rDK","colab_type":"code","colab":{}},"source":["# keep only those columns\n","df_train2 = df_train[['product_uid',\n","       'N_numerics_PT', 'N_numerics_ST', 'N_non_numerics_PT',\n","       'N_non_numerics_ST', 'N_common_words_leven',\n","       'JC_sim', 'N_substrs_PT_x',\n","       'N_substrs_PD_x', 'N_substrs_Atr_x',\n","       'Perc_substrs_x', 'N_substrs_PT_y',\n","       'N_substrs_PD_y', 'N_substrs_Atr_y', 'Perc_substrs_y',\n","       'Leven_sim_ST_PT', 'N_keywords_leven', 'relevance']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTo7izWj_rDL","colab_type":"code","colab":{}},"source":["df_train2.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQChEeYh_rDO","colab_type":"code","colab":{}},"source":["# save the 1st dataframe with features\n","df_train2.to_pickle(r'C:\\Users\\Dimos\\Desktop\\MSc\\Semester 4\\NLP\\Coursework\\trainsets\\df_train_ptstkwat.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLOnAC9X_rDR","colab_type":"text"},"source":["### -- Distances: Levenshtein, Cosine, Jaccard, Jaro"]},{"cell_type":"markdown","metadata":{"id":"lBDFOlCK_rDS","colab_type":"text"},"source":["#### --- Search term vs Product Title"]},{"cell_type":"code","metadata":{"id":"9Pj6Jfnt_rDS","colab_type":"code","colab":{}},"source":["# Levensthein distance between 'PT_text' & 'ST_text'\n","df_train['Leven_sim_PT'] = df_train.apply(lambda x: 1 - stringdist.levenshtein_norm(x['PT_text'], x['ST_text']),\n","                                         axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fo19Y2s__rDU","colab_type":"code","colab":{}},"source":["# Jaccard similarity\n","# Notes: εδω πιανουμε κοινες λεξεις που μπορει να εχουν διαφορετικες καταληξεις\n","def get_jaccard_sim(words1, words2): \n","    \"\"\"Returns jaccard similarity between 2 list of words\"\"\"\n","    try:\n","        a = set(words1) \n","        b = set(words2)\n","        c = a.intersection(b)\n","        return float(len(c)) / (len(a) + len(b) - len(c))\n","    except:\n","        return 0\n","\n","df_train['JC_sim_PT'] = df_train.apply(lambda x: get_jaccard_sim(x['PT_stem'], x['ST_stem']),\n","                                         axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Ewpd3F1_rDV","colab_type":"code","colab":{}},"source":["def get_cosine_sim(*strs):\n","    try:\n","        vectors = [t for t in get_vectors(*strs)]\n","        return cosine_similarity(vectors)[0][1]\n","    except:\n","        return 0\n","    \n","def get_vectors(*strs):\n","    text = [t for t in strs]\n","    vectorizer = CountVectorizer(text)\n","    vectorizer.fit(text)\n","    return vectorizer.transform(text).toarray()\n","\n","# cosine\n","df_train['Cosine_sim_PT'] = df_train.apply(lambda x: get_cosine_sim(' '.join(x['PT_stem']), ' '.join(x['ST_stem'])),\n","                                         axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f7WQUYPe_rDX","colab_type":"text"},"source":["#### --- Search term vs Description"]},{"cell_type":"code","metadata":{"id":"fjte_M0I_rDY","colab_type":"code","colab":{}},"source":["# Levensthein distance between 'Keywords_Descr' as text & 'ST_text'\n","df_train['Leven_sim_PD'] = df_train.apply(lambda x: 1 - stringdist.levenshtein_norm(' '.join(x['Keywords_Descr']), x['ST_text']),\n","                                         axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tzq_Sxdw_rDZ","colab_type":"code","colab":{}},"source":["# Jaccard similarity\n","df_train['JC_sim_PD'] = df_train.apply(lambda x: get_jaccard_sim(x['PD_stem'], x['ST_stem']),\n","                                         axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2URMRaE1_rDb","colab_type":"code","colab":{}},"source":["df_train['Cosine_sim_PD'] = df_train.apply(lambda x: get_cosine_sim(' '.join(x['PD_stem']), ' '.join(x['ST_stem'])),\n","                                         axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ZoFGNcv_rDf","colab_type":"text"},"source":["#### --- Search term vs Attributes"]},{"cell_type":"code","metadata":{"id":"0Ey9qyX9_rDg","colab_type":"code","colab":{}},"source":["df_train['Atrr_stem'] = df_train['Atrr_stem'].apply(lambda d: d if isinstance(d, list) else [])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"16UBFx0m_rDk","colab_type":"code","colab":{}},"source":["def get_leven(x):\n","    try:\n","        return 1 - stringdist.levenshtein_norm(' '.join(x['Atrr_stem']), x['ST_text'])\n","    except:\n","        #print('error')\n","        return 0\n","\n","# Levensthein distance between 'PT_text' & 'ST_text'\n","df_train['Leven_sim_Atrr'] = df_train.apply(lambda x: get_leven(x),\n","                                         axis=1)\n","# df_train['Leven_sim_Atrr'] = df_train.apply(lambda x: 1 - stringdist.levenshtein_norm(' '.join(x['Atrr_stem']), x['ST_text']),\n","#                                          axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRO5jrk8_rDn","colab_type":"code","colab":{}},"source":["# Jaccard similarity\n","df_train['JC_sim_Atrr'] = df_train.apply(lambda x: get_jaccard_sim(x['Atrr_stem'], x['ST_stem']),\n","                                         axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MwAJRq0V_rDu","colab_type":"code","colab":{}},"source":["df_train['Cosine_sim_Atrr'] = df_train.apply(lambda x: get_cosine_sim(' '.join(x['Atrr_stem']), ' '.join(x['ST_stem'])),\n","                                         axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RCBI908t_rDx","colab_type":"code","colab":{}},"source":["df_train.columns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXDVzkja_rDz","colab_type":"code","colab":{}},"source":["df_train_sims = df_train[['id', 'product_uid', 'JC_sim_PT', 'Cosine_sim_PT', 'Leven_sim_PD', 'JC_sim_PD',\n","       'Cosine_sim_PD', 'JC_sim_Atrr', 'Cosine_sim_Atrr', 'Leven_sim_Atrr']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AeUl5LnP_rD1","colab_type":"code","outputId":"9699ff92-bb5e-4196-d3a0-118475410184","colab":{}},"source":["df_train_sims.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>product_uid</th>\n","      <th>JC_sim_PT</th>\n","      <th>Cosine_sim_PT</th>\n","      <th>Leven_sim_PD</th>\n","      <th>JC_sim_PD</th>\n","      <th>Cosine_sim_PD</th>\n","      <th>JC_sim_Atrr</th>\n","      <th>Cosine_sim_Atrr</th>\n","      <th>Leven_sim_Atrr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>100001</td>\n","      <td>0.142857</td>\n","      <td>0.288675</td>\n","      <td>0.091743</td>\n","      <td>0.014085</td>\n","      <td>0.196960</td>\n","      <td>0.035714</td>\n","      <td>0.138675</td>\n","      <td>0.057143</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>100001</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.064220</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.040000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9</td>\n","      <td>100002</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.030075</td>\n","      <td>0.010204</td>\n","      <td>0.226134</td>\n","      <td>0.029412</td>\n","      <td>0.312348</td>\n","      <td>0.017937</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16</td>\n","      <td>100005</td>\n","      <td>0.076923</td>\n","      <td>0.182574</td>\n","      <td>0.123894</td>\n","      <td>0.015152</td>\n","      <td>0.063372</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17</td>\n","      <td>100005</td>\n","      <td>0.181818</td>\n","      <td>0.447214</td>\n","      <td>0.079646</td>\n","      <td>0.031250</td>\n","      <td>0.155230</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.050000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id product_uid  JC_sim_PT  Cosine_sim_PT  Leven_sim_PD  JC_sim_PD  \\\n","0  2   100001      0.142857   0.288675       0.091743      0.014085    \n","1  3   100001      0.000000   0.000000       0.064220      0.000000    \n","2  9   100002      0.000000   0.000000       0.030075      0.010204    \n","3  16  100005      0.076923   0.182574       0.123894      0.015152    \n","4  17  100005      0.181818   0.447214       0.079646      0.031250    \n","\n","   Cosine_sim_PD  JC_sim_Atrr  Cosine_sim_Atrr  Leven_sim_Atrr  \n","0  0.196960       0.035714     0.138675         0.057143        \n","1  0.000000       0.000000     0.000000         0.040000        \n","2  0.226134       0.029412     0.312348         0.017937        \n","3  0.063372       0.000000     0.000000         0.066667        \n","4  0.155230       0.000000     0.000000         0.050000        "]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"sQ_sBRue_rD4","colab_type":"code","colab":{}},"source":["# save the 2nd dataframe with features\n","df_train_sims.to_pickle(r'C:\\Users\\Dimos\\Desktop\\MSc\\Semester 4\\NLP\\Coursework\\trainsets\\df_similarities.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KjUVZMCp_rD6","colab_type":"text"},"source":["### -- Fuzzy matching"]},{"cell_type":"code","metadata":{"id":"yCAWfdSq_rD7","colab_type":"code","colab":{}},"source":["!pip install fuzzywuzzy\n","from fuzzywuzzy import fuzz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xk9xmVMm_rEA","colab_type":"code","outputId":"f6fbb3cb-2d28-4aa5-a730-00e402b9c594","executionInfo":{"status":"error","timestamp":1558823890563,"user_tz":-180,"elapsed":853,"user":{"displayName":"Dimitris Orfanoudakis","photoUrl":"","userId":"11488879855188947640"}},"colab":{"base_uri":"https://localhost:8080/","height":164}},"source":["df_train.head(5)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-557345c6c47b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"]}]},{"cell_type":"code","metadata":{"id":"vN3YHeN3_rED","colab_type":"code","outputId":"ddb9848d-3e19-4b98-f599-762672ba0905","executionInfo":{"status":"ok","timestamp":1558823894057,"user_tz":-180,"elapsed":1407,"user":{"displayName":"Dimitris Orfanoudakis","photoUrl":"","userId":"11488879855188947640"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# examples\n","#https://towardsdatascience.com/natural-language-processing-for-fuzzy-string-matching-with-python-6632b7824c49\n","fuzz.ratio(\"this is a test\", \"this is a test!\")\n","fuzz.partial_ratio(\"this is a test\", \"this is a test!\")\n","fuzz.ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")\n","fuzz.token_sort_ratio(\"fuzzy wuzzy was a bear\", \"wuzzy fuzzy was a bear\")\n","fuzz.token_sort_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")\n","fuzz.token_set_ratio(\"fuzzy was a bear\", \"fuzzy fuzzy was a bear\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["100"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"NCYS29p6_rEP","colab_type":"code","outputId":"c6448207-5be0-49af-86ff-7133ccee286e","executionInfo":{"status":"error","timestamp":1558823917687,"user_tz":-180,"elapsed":1095,"user":{"displayName":"Dimitris Orfanoudakis","photoUrl":"","userId":"11488879855188947640"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["# product title & search_term\n","df_train['FZ_PT_1'] = df_train.apply(lambda x: fuzz.ratio(x['PT_lower'], x['ST_lower']), axis=1)\n","df_train['FZ_PT_2'] = df_train.apply(lambda x: fuzz.partial_ratio(x['PT_lower'], x['ST_lower']), axis=1)\n","df_train['FZ_PT_3'] = df_train.apply(lambda x: fuzz.token_sort_ratio(x['PT_lower'], x['ST_lower']), axis=1)\n","df_train['FZ_PT_4'] = df_train.apply(lambda x: fuzz.token_set_ratio(x['PT_lower'], x['ST_lower']), axis=1)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-9d2a3e5ef66a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FZ_PT_1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PT_lower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ST_lower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FZ_PT_2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PT_lower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ST_lower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FZ_PT_3'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_sort_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PT_lower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ST_lower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FZ_PT_4'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_set_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PT_lower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ST_lower'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"]}]},{"cell_type":"code","metadata":{"id":"_6Djw0Jt_rER","colab_type":"code","colab":{}},"source":["# convert 'Atrr_text_all' column to strings in order to avoid errors\n","df_train['Atrr_text_all'] = df_train['Atrr_text_all'].astype('str')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zO6N2lox_rEU","colab_type":"code","colab":{}},"source":["# product title & search_term\n","df_train['FZ_Attr_1'] = df_train.apply(lambda x: fuzz.ratio(x['Atrr_text_all'], x['ST_lower']), axis=1)\n","df_train['FZ_Attr_2'] = df_train.apply(lambda x: fuzz.partial_ratio(x['Atrr_text_all'], x['ST_lower']), axis=1)\n","df_train['FZ_Attr_3'] = df_train.apply(lambda x: fuzz.token_sort_ratio(x['Atrr_text_all'], x['ST_lower']), axis=1)\n","df_train['FZ_Attr_4'] = df_train.apply(lambda x: fuzz.token_set_ratio(x['Atrr_text_all'], x['ST_lower']), axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-5w41b4_rEX","colab_type":"code","outputId":"42cd9ba1-e93e-4eb4-88e9-690610aa6647","colab":{}},"source":["df_train.columns"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['id', 'product_uid', 'product_title', 'search_term', 'relevance',\n","       'PT_lower', 'PT_tokens', 'PT_tokens_sw', 'PT_text', 'PT_stem',\n","       'PT_numerics', 'PT_Non_numerics', 'ST_lower', 'ST_tokens',\n","       'ST_tokens_sw', 'ST_text', 'ST_stem', 'ST_numerics', 'ST_Non_numerics',\n","       'PD_lower', 'PD_stem', 'PD_numerics', 'Keywords_Descr', 'Atrr_text_all',\n","       'Atrr_stem', 'FZ_PT_1', 'FZ_PT_2', 'FZ_PT_3', 'FZ_PT_4', 'FZ_Attr_1',\n","       'FZ_Attr_2', 'FZ_Attr_3', 'FZ_Attr_4'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"DBLke-qI_rEd","colab_type":"code","colab":{}},"source":["df_fuzzy = df_train[['product_uid', 'FZ_PT_1', 'FZ_PT_2', 'FZ_PT_3', 'FZ_PT_4', 'FZ_Attr_1',\n","       'FZ_Attr_2', 'FZ_Attr_3', 'FZ_Attr_4']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-Xjbp52_rEf","colab_type":"code","outputId":"d37baa37-624a-4688-ee92-edbb18f1cf58","colab":{}},"source":["df_fuzzy.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style>\n","    .dataframe thead tr:only-child th {\n","        text-align: right;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: left;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>product_uid</th>\n","      <th>FZ_PT_1</th>\n","      <th>FZ_PT_2</th>\n","      <th>FZ_PT_3</th>\n","      <th>FZ_PT_4</th>\n","      <th>FZ_Attr_1</th>\n","      <th>FZ_Attr_2</th>\n","      <th>FZ_Attr_3</th>\n","      <th>FZ_Attr_4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>100001</td>\n","      <td>22</td>\n","      <td>56</td>\n","      <td>39</td>\n","      <td>56</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>4</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>100001</td>\n","      <td>10</td>\n","      <td>36</td>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>11</td>\n","      <td>3</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100002</td>\n","      <td>18</td>\n","      <td>89</td>\n","      <td>19</td>\n","      <td>19</td>\n","      <td>2</td>\n","      <td>67</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>100005</td>\n","      <td>26</td>\n","      <td>62</td>\n","      <td>26</td>\n","      <td>55</td>\n","      <td>4</td>\n","      <td>62</td>\n","      <td>4</td>\n","      <td>81</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>100005</td>\n","      <td>38</td>\n","      <td>100</td>\n","      <td>38</td>\n","      <td>100</td>\n","      <td>6</td>\n","      <td>67</td>\n","      <td>6</td>\n","      <td>100</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  product_uid  FZ_PT_1  FZ_PT_2  FZ_PT_3  FZ_PT_4  FZ_Attr_1  FZ_Attr_2  \\\n","0  100001      22       56       39       56       4          8           \n","1  100001      10       36       10       10       2          11          \n","2  100002      18       89       19       19       2          67          \n","3  100005      26       62       26       55       4          62          \n","4  100005      38       100      38       100      6          67          \n","\n","   FZ_Attr_3  FZ_Attr_4  \n","0  4          5          \n","1  3          4          \n","2  1          1          \n","3  4          81         \n","4  6          100        "]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"uqjPtL6R_rEl","colab_type":"code","colab":{}},"source":["# save the 3rd dataframe with features\n","df_fuzzy.to_pickle(r'C:\\Users\\Dimos\\Desktop\\MSc\\Semester 4\\NLP\\Coursework\\trainsets\\df_fuzzy.pkl')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xI_2fQQh_rEt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}